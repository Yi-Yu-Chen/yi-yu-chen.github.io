<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <!-- MathJax for equations, if needed -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <title>Dynamic Light Video Generation</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 40px;
      background-color: #f4f4f4;
      color: #333;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }
    figure {
      margin: 1em 0;
      text-align: center;
    }
    figcaption {
      font-size: 0.9em;
      color: #444;
    }
    img {
      max-width: 100%;
      height: auto;
      box-shadow: 0 2px 6px rgba(0,0,0,0.1);
      border-radius: 4px;
    }
    code {
      background-color: #e0e0e0;
      font-family: monospace;
      font-size: 14px;
      border-radius: 4px;
      padding: 2px;    
    }
    h1, h2, h3, h4 {
      margin-top: 1.2em;
      margin-bottom: 0.6em;
    }
    .video-container {
      text-align: center; 
      margin: 20px 0; 
    }
    .gallery-2 {
        display: grid;
        grid-template-columns: repeat(2, 1fr); 
        margin-top: 20px;
        justify-items: center;
        text-align: center;
    }
    .gallery-3 {
        display: grid;
        grid-template-columns: repeat(3, 1fr); 
        margin-top: 20px;
        justify-items: center
    }
    .gallery-4 {
        display: grid;
        grid-template-columns: repeat(4, 1fr); 
        margin-top: 20px;
        justify-items: center
    }
    .gallery-5 {
        display: grid;
        margin: 0 auto; 
        grid-template-columns: repeat(5, 1fr); 
        margin-top: 20px;
        justify-items: center
    }
    .gallery-3 img {
        width: 100%;
        height: auto;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1); 
    }
    .gallery-4 img {
        width: 90%;
        height: auto;
        align-self: center;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1); 
    }
    .gallery-5 img {
        width: 100%;
        height: auto;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1); 
    }
  </style>
</head>

<body>
  <h1>Dynamic Light Video Generation: Extending Spinelli’s Interactive Art with Sound</h1>
  <p><em>By Yiyu Chen and Yuqin Jiao</em></p>

  <h2>Final Result Video</h2>
  <div class="video-container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>

  <div class="video-container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>

  <div class="video-container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>
  <p>
    <a href="https://docs.google.com/presentation/d/1dm8nVjpUkTWvmHO4dTRlXgqP7O5xhCs27SJNx3efdpM/edit?usp=sharing">Our Presentation Slide</a>
  </p>


  <h2>Abstract</h2>
  <p>
    We explore a new way of synchronizing vivid, pulsating light trails with musical sound waves, 
    inspired by <a href="https://www.luceaspinelli.com/motion/#itemId=59a9b9e6d7bdce5d146f72c8">Lucea Spinelli's</a> light-based art. By building a two-stage, image-to-image pipeline, 
    our system dynamically reacts to audio input, generating luminescent trails that preserve the 
    underlying background. These light effects are generated by combining audio feature extraction, 
    diffusion-based image synthesis, and fine-tuning using a DreamBooth model. The result is an 
    immersive audiovisual experience, showcasing a fresh approach to interactive digital art.
  </p>

  <h2>1. Introduction</h2>
  <p>
    Dynamic visualizations tied to musical elements can appear in concerts, interactive art 
    installations, and multimedia entertainment. Traditional work often directly maps waveforms 
    to visual indicators, but more advanced approaches can elevate the aesthetic experience.
    Inspired by Lucea Spinelli's photography that captures light in motion, we wanted to create 
    a system where light patterns become alive in step with a musical soundtrack.
  </p>
  <div class="gallery-2">
  <figure>
    <img src="img/Isle+800w.gif" alt="time-lapse inside a train">
    <figcaption>Time-lapse inside a train (Artwork by Lucea Spinelli)</figcaption>
  </figure>
  <figure>
    <img src="img/MFAVN-2-bench-arches-700w.gif" alt="time-lapse on a bench">
    <figcaption>Time-lapse on a bench (Artwork by Lucea Spinelli)</figcaption>
  </figure>
</div>
<p> <a href="https://www.luceaspinelli.com/motion/#itemId=59a9b9e6d7bdce5d146f72c8">More Lucea Spinelli's Artwork</a></p>

  <p>
    Our approach begins by extracting musical features such as volume (RMS energy) and rhythm (beat 
    detection). We use these parameters to guide where and how bright the light effects will appear. 
    Next, a two-stage pipeline (based on latent diffusion) produces the background with pulsating 
    trails. We also apply a specialized DreamBooth fine-tuning step so that the final visuals 
    emulate Spinelli’s signature style. Finally, additional rendering logic keeps the background 
    consistent while adjusting brightness and color for the new light trails.
  </p>

 
  <h2>2. Method</h2>
  <p>
    Our system processes each video frame (or image) through three main steps: </p>
    <ol>
        <li>(1) Audio-driven Preprocessing </li>
        <li>(2) A Two-Stage Image-to-Image Diffusion Pipeline</li>
        <li>(3) Light Effect Rendering</li>
    </ol>    

  <figure>
    <img src="img/flow_graph_1.jpg" alt="Pipeline Flow Diagram" />
    <figcaption>Overview of our light trail generation pipeline</figcaption>
  </figure>

  <h3>2.1 Preprocessing</h3>
  <p>
    We extract two vital musical properties: (1) the volume (RMS) and (2) the beat. Volume guides 
    how strong or intense the light should be, while beat detection finds key pulses in the music 
    (e.g., drum hits). A Gaussian filter smooths these signals. We combine them into a 
    <code>Brightness</code> feature:
  </p>
  <div>
    <p>
    \[ \text{Brightness} = w_{\text{volume}} \times \text{RMS} + w_{\text{beat}} \times \text{Beat} \] </p>
  </div>
  <p>
    This brightness value triggers how many and how bright the light trails become in each frame.
  </p>
  <figure>
    <img src="img/Audio Chat.png" alt="Visualization of audio features" />
    <figcaption>Visualization of audio features and "brightness"</figcaption>
  </figure>


  <h4>Waveform-Based Light Trails</h4>
  <p>
    One way to visualize the audio is to draw “light scribbles” following the waveform. By adding 
    random noise over time, it simulates flickering lights. Higher brightness means more 
    overlapping trails and stronger glow.
  </p>
  <div class="gallery-2">
    <figure>
        <img src="img/smooth.png" alt="Visualization of audio features" />
        <figcaption>Smoothing Audoo Waveform of First Frame</figcaption>
    </figure>
    <figure>
        <img src="img/all waveform.png" alt="Visualization of audio features" />
        <figcaption>3 Waveform after Adding Noise </figcaption>
    </figure>
    <figure>
        <img src="img/waveform with offset.png" alt="Visualization of audio features" />
        <figcaption>3 Waveform after Adding Offset to seperate</figcaption>
    </figure>
    <figure>
        <img src="img/frame_000.png" alt="Visualization of audio features" />
        <figcaption>Rotation and Plot to Background Image</figcaption>
    </figure>
    </div>

  <h4>Structured Motion Line Visualization</h4>
  <p>
    Alternatively, we can generate consistent lines or arcs that move in a more structured manner. 
    The brightness still decides how vivid or visible these lines appear, preserving a more 
    geometric style compared to the scribble-like waveform method.
  </p>

  <h3>2.2 DreamBooth Fine-Tuning</h3>
  <p>
    To capture the distinct aesthetic of Spinelli’s photography, we fine-tuned Stable Diffusion 
    using around 67 curated images. Each image highlights characteristic light trails or patterns. 
    During training, we used prompts focusing on these elements, a tuned learning rate, and 
    relatively high-resolution outputs for clarity.
  </p>
  <p>
    After training, the model can produce new images with that unique "light trail" look. For 
    instance, a prompt like <code>"a bunch of curly light trails from time-lapse inside a train"</code> 
    can now faithfully mimic Spinelli’s style for any new background context.
  </p>

  <h3>2.3 Two-Stage Custom Pipeline</h3>
  <p>
    Our core pipeline is split into two diffusion stages. The first stage runs a conventional 
    diffusion from noise to a rough image (or from a partially noised background to a partially 
    reconstructed image). Once we have a draft, we apply a custom mask so that subsequent steps 
    only modify certain areas—namely, the regions designated for light trails.
  </p>
  <p>
    Mathematically, we apply the usual denoising step:
  </p>
</p>
<div>
  <p>
    \[
    x_{t-1} = \sqrt{\alpha_{t-1}} \left(\frac{x_t - \sqrt{1-\alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}}\right) + \sigma_t \epsilon
    \]
</p>
where \(X_t\) represents the image at time step \(t\), \(\epsilon_\theta\) denotes the neural network predicting noise, and \(\alpha_t\) and \(\sigma_t\) are parameters controlling the noise levels across the diffusion steps.
</div>
<p>
  <p>
    Then for each subsequent iteration, we selectively keep or refine parts of the latent 
    representation based on the custom mask:
  </p>

    <div>
        <p>
            \[
            x_{t-1}^m = \sqrt{\alpha_{t-1}} \left(\frac{x_t - \sqrt{1-\alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}}\right) + \sigma_t \epsilon^m
            \]
            \[
            x_{t-1} = x_{t-1}^m \odot m + x_{t'} \odot (1 - m)
            \]
        </p>
  <p>
    Here, \(x_t\) represents the latent state of the image at time step \( t \) , \(x_{t - 1}^m \) is the latent representation of the image in the masked areas after denoising at timestep \(t -1\), \(x_{t'}\) is the preserved latent state from the step when the mask was first applied, and \( m \)is the custom mask. The operator \(\odot\) indicates element-wise multiplication, which allows the diffusion process to modify the image selectively as per the mask's guidance.
    </p>
    <p>
    This ensures the original background remains intact, while areas in the mask (i.e., where 
    trails should exist) evolve with the diffusion process.
  </p>

  <figure>
    <img src="img/two_stage_2.jpg" alt="Two-stage pipeline visualization" />
    <figcaption>Visualization of our two-stage pipeline and masked refinement</figcaption>
  </figure>

  <h3>2.4 Light Effect Rendering</h3>
  <p>
    Even after diffusion, there can be discrepancies in brightness or color between the new 
    light trails and the base photo. Inspired by classic methods for inserting synthetic objects 
    into real scenes, we measure local luminance in the masked region, compare it to the global 
    background, and adjust accordingly.
  </p>
  <div class="gallery-3">
    <figure>
        <img src="img/train00.jpg" alt="original image" />
        <figcaption>Original image</figcaption>
    </figure>
    <figure>
        <img src="img/output_005_1.png" alt="model output image" />
        <figcaption>Model output (raw image with trails)</figcaption>
    </figure>
    <figure>
        <img src="img/output_005_2.png" alt="final image" />
        <figcaption>Final rendered image after brightness balancing</figcaption>
    </figure>
    </div>
  <p>
    We compute local scene luminance (<em>LS</em><sub>obj</sub>), global scene luminance 
    (<em>LS</em><sub>noobj</sub>), and total original luminance (<em>LS</em><sub>total</sub>). 
    Then we adjust the final background by:
  </p>
  <div>
    <p>
        \[
        LS_{\text{final}} = LS_{\text{total}} - (LS_{\text{obj}} - LS_{\text{noobj}})
        \]
    </p>
  </div>
  <p>
    We also scale by comparing average brightness levels between the model’s output and the original 
    photo. This maintains a cohesive look, ensuring bright trails don’t unrealistically “bleach” 
    the background or, conversely, appear too dim.
  </p>

  <h2>3. Conclusion</h2>
  <p>
    We presented a method for synchronizing swirling, imaginative light trails with music, 
    inspired by Spinelli’s aesthetic. Our pipeline is based on a DreamBooth-fine-tuned diffusion 
    model and a two-stage process that carefully preserves background geometry. 
  </p>
  <p>
    This work broadens the potential for interactive media. It suggests new ways to animate 
    photography or video in harmony with sound, from immersive installations to digital 
    performance art. Future directions include real-time generation for live shows, more 
    advanced temporal consistency, and expansions to other visual styles.
  </p>

</body>
</html>
